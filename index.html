<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Paper Reading List</title>
  
  <meta name="author" content="PaperReadingList">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="image/arXiv.png">
</head>

  <body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
 

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <heading>&nbsp; &nbsp; &nbsp;Reading List</heading>

		
	      

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td width="15%">
        <a href="paper/NeurIPS2022_PI/introduction.png"><img src='paper/NeurIPS2022_PI/introduction.png'  width="200" height="110"></a>
        </td>
      <td valign="top" width="75%">
	 <a name="[10]"><big>[10]</big></a> <em>Retrieve, Reason, and Refine: Generating Accurate and Faithful Patient Instructions</em><br>
     <strong>Fenglin Liu</strong>, Bang Yang, Chenyu You, Xian Wu, Shen Ge, Zhangdaihong Liu, Xu Sun, Yang Yang, David A. Clifton<br>
        In Proceedings of <font color="#a82e2e"><strong>NeurIPS 2022</strong></font>,
	      <a href="https://openreview.net/forum?id=dp0zWsdOV1h">[paper link]</a>, 
	      <a href="paper/NeurIPS2022_PI/NeurIPS2022_PI.pdf">[pdf]</a>,
	      <a href="">[ppt]</a>,
	      <a href="paper/NeurIPS2022_PI/NeurIPS2022_Appendix.pdf">[appendix]</a>,
	      <a href="paper/NeurIPS2022_PI/nips2022_poster.pdf">[poster]</a>,
	      <a href="https://github.com/AI-in-Hospitals/Patient-Instructions">[code]</a><br>
	      
        <p></p>
		<p>&bull; We propose a new task of Patient Instruction (PI) generation which attempts to generate accurate and faithful PIs,
			which guide the patients how to manage their conditions after discharge based on their health records during hospitalization.
		To address this task, we build a dataset PI and present an effective approach Re<sup>3</sup>Writer: Retrieve, Reason, and Refine.</p>
	</td>
    </tr>
   </table>

	      
 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td width="15%">
        <a href="paper/NeurIPS2022_CATformer/introduction.png"><img src='paper/NeurIPS2022_CATformer/introduction.png'  width="200" height="110"></a>
        </td>
      <td valign="top" width="75%">
	 <a name="[9]"><big>[9]</big></a> <em>Class-Aware Generative Adversarial Transformers for Medical Image Segmentation</em><br>
     Chenyu You, Ruihan Zhao, <strong>Fenglin Liu</strong>, Sandeep Chinchali, Ufuk Topcu, Lawrence Staib, James S Duncan<br>
        In Proceedings of <font color="#a82e2e"><strong>NeurIPS 2022</strong></font>,
	      <a href="https://openreview.net/forum?id=aqLugNVQqRw">[paper link]</a>, 
	      <a href="paper/NeurIPS2022_CATformer/NeurIPS2022_CATformer.pdf">[pdf]</a>, 
	      <a href="">[poster]</a>,
	      <a href="">[code]</a><br>
	      
        <p></p>
		<p>&bull; We make the first attempt to build a GAN using a transformer-based architecture for the 2D medical image segmentation task.
			We incorporate the pyramid structure into the generator to learn rich global and local multi-scale spatial representations, 
			and also devise a novel class-aware transformer module by progressively learning the interesting regions correlated with the semantic structures of images.</p>
	</td>
    </tr>
   </table>
	     
	      
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td width="15%">
        <a href="paper/NeurIPS2022_EMCL/introduction.png"><img src='paper/NeurIPS2022_EMCL/introduction.png'  width="200" height="110"></a>
        </td>
      <td valign="top" width="75%">
	 <a name="[8]"><big>[8]</big></a> <em>Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations</em><br>
     Peng Jin, JinFa Huang, <strong>Fenglin Liu</strong>, Xian Wu, Shen Ge, Guoli Song, David A. Clifton, Jie Chen<br>
        In Proceedings of <font color="#a82e2e"><strong>NeurIPS 2022</font> (<font color="red">Spotlight</font>)</strong>,
	      <a href="https://openreview.net/forum?id=ijzm0EhAY_w">[paper link]</a>, 
	      <a href="paper/NeurIPS2022_EMCL/NeurIPS2022_EMCL.pdf">[pdf]</a>, 
	      <a href="paper/NeurIPS2022_EMCL/nips2022_poster_emcl.png">[poster]</a>,
	      <a href="https://github.com/jpthu17/EMCL">[code]</a><br>
	      
        <p></p>
		<p>&bull; To alleviate the cross-modal representation bias, we reformulate the contrastive learning for video-and-language representations
			into an expectation-maximization iteration manner and propose a plug-and-play feature projection module named Expectation-Maximization Contrastive Learning (EMCL),
			which learns the subspace that aims to become semantic-relevant and compact representation.</p>
	</td>
    </tr>
   </table>




    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">

     <a href="paper/TPAMI2022_UVC-VI/framework.png"><img src='paper/TPAMI2022_UVC-VI/framework.png' width="200" height="110"></a>

      </td>
      <td valign="top" width="75%">
	 <a name="[7]"><big>[7]</big></a> <em>Aligning Source Visual and Target Language Domains for Unpaired Video Captioning</em><br>
     <strong>Fenglin Liu</strong>, Xian Wu, Chenyu You, Shen Ge, Yuexian Zou, Xu Sun<br>
        IEEE Transactions on Pattern Analysis and Machine Intelligence <strong>(<font color="#a82e2e">TPAMI 2022</font>, <font color="red">IF: 24.314</font>)</strong>,
                <a href="https://ieeexplore.ieee.org/document/9633156">[paper link]</a>, 
	        <a href="paper/TPAMI2022_UVC-VI/TPAMI2022__Unpaired_Video_Captioning.pdf">[pdf]</a><br>
	      
        <p></p>
		<p>&bull; <em>We make the first attempt to conduct unpaired video captioning under various low-resource language application scenarios, 
			e.g., French, German and Chinese, in which the video-caption pairs are not available</em>. <br>
		&bull; We present the Unpaired Video Captioning with Visual Injection system, which even exceeds several recently proposed supervised systems.</p>
	</td> 
    </tr>
   </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td width="15%">
        <a href="paper/NeurIPS2021_KGAE/framework.png"><img src='paper/NeurIPS2021_KGAE/framework.png'  width="200" height="110"></a>
        </td>
      <td valign="top" width="75%">
	 <a name="[6]"><big>[6]</big></a> <em>Auto-Encoding Knowledge Graph for Unsupervised Medical Report Generation</em><br>
     <strong>Fenglin Liu</strong>, Chenyu You, Xian Wu, Shen Ge, Sheng Wang, Xu Sun<br>
        In Proceedings of <font color="#a82e2e"><strong>NeurIPS 2021</strong></font>,
	      <a href="https://proceedings.neurips.cc/paper/2021/hash/876e1c59023b1a0e95808168e1a8ff89-Abstract.html">[paper link]</a>, 
	      <a href="paper/NeurIPS2021_KGAE/NeurIPS2021_KGAE.pdf">[pdf]</a>, 
	      <a href="paper/NeurIPS2021_KGAE/nips21_poster.pdf">[poster]</a><br>
	      
        <p></p>
		<p>&bull; We make the first attempt to train a medical report generation model in an unsupervised manner. <br>
		&bull; We propose the Knowledge Graph Auto-Encoder (KGAE). <em>Without any image-report pairs</em>, 
			KGAE can extract the knowledge representations of both image and report from the knowledge graph to bridge the visual and textual domains, 
			and generate desirable reports by being trained in the auto-encoding pipeline. </p>
	</td>
    </tr>
   </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">

     <a href="paper/CVPR2021_PPKED/framework.png"><img src='paper/CVPR2021_PPKED/framework.png' width="200" height="110"></a>

      </td>
      <td valign="top" width="75%">
	 <a name="[5]"><big>[5]</big></a> <em>Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation</em><br>
     <strong>Fenglin Liu</strong>, Xian Wu, Shen Ge, Wei Fan, Yuexian Zou<br>
        In Proceedings of <font color="#a82e2e"><strong>CVPR 2021</strong></font>,
	      <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Exploring_and_Distilling_Posterior_and_Prior_Knowledge_for_Radiology_Report_CVPR_2021_paper.pdf">[paper link]</a>, 
	      <a href="paper/CVPR2021_PPKED/CVPR2021_PPKED.pdf">[pdf]</a>, 
	      <a href="paper/CVPR2021_PPKED/CVPR2021_PPKED_ppt.pdf">[ppt]</a>, 
	      <a href="paper/CVPR2021_PPKED/CVPR2021_PPKED_poster.pdf">[poster]</a><br>
	      
        <p></p>
		<p>&bull; We propose to explore and distill posterior and prior knowledge by first examining the abnormal regions, and then relying on the prior medical knowledge and prior working experience to write accurate radiology reports. <br>
		&bull;  Media Coverage (<strong><font color="red">17,000 reads</font></strong>, in Chinese): 
			<img class="img-thumbnail" src="image/liangziwei.jpg" alt="" height="16"><a href="https://mp.weixin.qq.com/s/zGBpTyycIMCkXJBrEdYdfA"> QbitAI (量子位)</a>; 
			<img class="img-thumbnail" src="image/jiqizhixin.jpg" alt="" style="" height="16"><a href="https://mp.weixin.qq.com/s/4Tp5ReW0tHozWX1hKexzEQ"> Synced (机器之心)</a>.</p>
      </td>
    </tr>
   </table>
    
 

		

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<br>
				<p align="middle"><font size="1">
				<a href="http://www.clustrmaps.com/map/Paperlist.site" title="Visit tracker for Paperlist.site">
					<img src="//www.clustrmaps.com/map_v2.png?d=aahU2d7YM1co9Ob0B_5uxAcswOX3gGWBAlxDpPjsmy8" /></a>
				<br/> <br/>
			
				</tbody></table>
 
	
				
	 
				
				
  </body>
</html>

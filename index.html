<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Paper Reading List</title>
  
  <meta name="author" content="Fenglin Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="image/oxford.png">
</head>

  <body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
 

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <a name="pub"> </a><heading>&nbsp; &nbsp; &nbsp;üìù Publications</heading>
	      <strongsmall>
			<a href="https://scholar.google.com/citations?user=eTEITdwAAAAJ"><strongsmall>[Google Scholar]</strongsmall></a>
		</strongsmall>
	      

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td width="15%">
        <a href="paper/NeurIPS2022_PI/introduction.png"><img src='paper/NeurIPS2022_PI/introduction.png'  width="200" height="110"></a>
        </td>
      <td valign="top" width="75%">
	 <a name="[10]"><big>[10]</big></a> <em>Retrieve, Reason, and Refine: Generating Accurate and Faithful Patient Instructions</em><br>
     <strong>Fenglin Liu</strong>, Bang Yang, Chenyu You, Xian Wu, Shen Ge, Zhangdaihong Liu, Xu Sun, Yang Yang, David A. Clifton<br>
        In Proceedings of <font color="#a82e2e"><strong>NeurIPS 2022</strong></font>,
	      <a href="https://openreview.net/forum?id=dp0zWsdOV1h">[paper link]</a>, 
	      <a href="paper/NeurIPS2022_PI/NeurIPS2022_PI.pdf">[pdf]</a>,
	      <a href="">[ppt]</a>,
	      <a href="paper/NeurIPS2022_PI/NeurIPS2022_Appendix.pdf">[appendix]</a>,
	      <a href="paper/NeurIPS2022_PI/nips2022_poster.pdf">[poster]</a>,
	      <a href="https://github.com/AI-in-Hospitals/Patient-Instructions">[code]</a><br>
	      
        <p></p>
		<p>&bull; We propose a new task of Patient Instruction (PI) generation which attempts to generate accurate and faithful PIs,
			which guide the patients how to manage their conditions after discharge based on their health records during hospitalization.
		To address this task, we build a dataset PI and present an effective approach Re<sup>3</sup>Writer: Retrieve, Reason, and Refine.</p>
	</td>
    </tr>
   </table>

	      
 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td width="15%">
        <a href="paper/NeurIPS2022_CATformer/introduction.png"><img src='paper/NeurIPS2022_CATformer/introduction.png'  width="200" height="110"></a>
        </td>
      <td valign="top" width="75%">
	 <a name="[9]"><big>[9]</big></a> <em>Class-Aware Generative Adversarial Transformers for Medical Image Segmentation</em><br>
     Chenyu You, Ruihan Zhao, <strong>Fenglin Liu</strong>, Sandeep Chinchali, Ufuk Topcu, Lawrence Staib, James S Duncan<br>
        In Proceedings of <font color="#a82e2e"><strong>NeurIPS 2022</strong></font>,
	      <a href="https://openreview.net/forum?id=aqLugNVQqRw">[paper link]</a>, 
	      <a href="paper/NeurIPS2022_CATformer/NeurIPS2022_CATformer.pdf">[pdf]</a>, 
	      <a href="">[poster]</a>,
	      <a href="">[code]</a><br>
	      
        <p></p>
		<p>&bull; We make the first attempt to build a GAN using a transformer-based architecture for the 2D medical image segmentation task.
			We incorporate the pyramid structure into the generator to learn rich global and local multi-scale spatial representations, 
			and also devise a novel class-aware transformer module by progressively learning the interesting regions correlated with the semantic structures of images.</p>
	</td>
    </tr>
   </table>
	     
	      
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td width="15%">
        <a href="paper/NeurIPS2022_EMCL/introduction.png"><img src='paper/NeurIPS2022_EMCL/introduction.png'  width="200" height="110"></a>
        </td>
      <td valign="top" width="75%">
	 <a name="[8]"><big>[8]</big></a> <em>Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations</em><br>
     Peng Jin, JinFa Huang, <strong>Fenglin Liu</strong>, Xian Wu, Shen Ge, Guoli Song, David A. Clifton, Jie Chen<br>
        In Proceedings of <font color="#a82e2e"><strong>NeurIPS 2022</font> (<font color="red">Spotlight</font>)</strong>,
	      <a href="https://openreview.net/forum?id=ijzm0EhAY_w">[paper link]</a>, 
	      <a href="paper/NeurIPS2022_EMCL/NeurIPS2022_EMCL.pdf">[pdf]</a>, 
	      <a href="paper/NeurIPS2022_EMCL/nips2022_poster_emcl.png">[poster]</a>,
	      <a href="https://github.com/jpthu17/EMCL">[code]</a><br>
	      
        <p></p>
		<p>&bull; To alleviate the cross-modal representation bias, we reformulate the contrastive learning for video-and-language representations
			into an expectation-maximization iteration manner and propose a plug-and-play feature projection module named Expectation-Maximization Contrastive Learning (EMCL),
			which learns the subspace that aims to become semantic-relevant and compact representation.</p>
	</td>
    </tr>
   </table>




    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">

     <a href="paper/TPAMI2022_UVC-VI/framework.png"><img src='paper/TPAMI2022_UVC-VI/framework.png' width="200" height="110"></a>

      </td>
      <td valign="top" width="75%">
	 <a name="[7]"><big>[7]</big></a> <em>Aligning Source Visual and Target Language Domains for Unpaired Video Captioning</em><br>
     <strong>Fenglin Liu</strong>, Xian Wu, Chenyu You, Shen Ge, Yuexian Zou, Xu Sun<br>
        IEEE Transactions on Pattern Analysis and Machine Intelligence <strong>(<font color="#a82e2e">TPAMI 2022</font>, <font color="red">IF: 24.314</font>)</strong>,
                <a href="https://ieeexplore.ieee.org/document/9633156">[paper link]</a>, 
	        <a href="paper/TPAMI2022_UVC-VI/TPAMI2022__Unpaired_Video_Captioning.pdf">[pdf]</a><br>
	      
        <p></p>
		<p>&bull; <em>We make the first attempt to conduct unpaired video captioning under various low-resource language application scenarios, 
			e.g., French, German and Chinese, in which the video-caption pairs are not available</em>. <br>
		&bull; We present the Unpaired Video Captioning with Visual Injection system, which even exceeds several recently proposed supervised systems.</p>
	</td> 
    </tr>
   </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
        <td width="15%">
        <a href="paper/NeurIPS2021_KGAE/framework.png"><img src='paper/NeurIPS2021_KGAE/framework.png'  width="200" height="110"></a>
        </td>
      <td valign="top" width="75%">
	 <a name="[6]"><big>[6]</big></a> <em>Auto-Encoding Knowledge Graph for Unsupervised Medical Report Generation</em><br>
     <strong>Fenglin Liu</strong>, Chenyu You, Xian Wu, Shen Ge, Sheng Wang, Xu Sun<br>
        In Proceedings of <font color="#a82e2e"><strong>NeurIPS 2021</strong></font>,
	      <a href="https://proceedings.neurips.cc/paper/2021/hash/876e1c59023b1a0e95808168e1a8ff89-Abstract.html">[paper link]</a>, 
	      <a href="paper/NeurIPS2021_KGAE/NeurIPS2021_KGAE.pdf">[pdf]</a>, 
	      <a href="paper/NeurIPS2021_KGAE/nips21_poster.pdf">[poster]</a><br>
	      
        <p></p>
		<p>&bull; We make the first attempt to train a medical report generation model in an unsupervised manner. <br>
		&bull; We propose the Knowledge Graph Auto-Encoder (KGAE). <em>Without any image-report pairs</em>, 
			KGAE can extract the knowledge representations of both image and report from the knowledge graph to bridge the visual and textual domains, 
			and generate desirable reports by being trained in the auto-encoding pipeline. </p>
	</td>
    </tr>
   </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">

     <a href="paper/CVPR2021_PPKED/framework.png"><img src='paper/CVPR2021_PPKED/framework.png' width="200" height="110"></a>

      </td>
      <td valign="top" width="75%">
	 <a name="[5]"><big>[5]</big></a> <em>Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation</em><br>
     <strong>Fenglin Liu</strong>, Xian Wu, Shen Ge, Wei Fan, Yuexian Zou<br>
        In Proceedings of <font color="#a82e2e"><strong>CVPR 2021</strong></font>,
	      <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Exploring_and_Distilling_Posterior_and_Prior_Knowledge_for_Radiology_Report_CVPR_2021_paper.pdf">[paper link]</a>, 
	      <a href="paper/CVPR2021_PPKED/CVPR2021_PPKED.pdf">[pdf]</a>, 
	      <a href="paper/CVPR2021_PPKED/CVPR2021_PPKED_ppt.pdf">[ppt]</a>, 
	      <a href="paper/CVPR2021_PPKED/CVPR2021_PPKED_poster.pdf">[poster]</a><br>
	      
        <p></p>
		<p>&bull; We propose to explore and distill posterior and prior knowledge by first examining the abnormal regions, and then relying on the prior medical knowledge and prior working experience to write accurate radiology reports. <br>
		&bull;  Media Coverage (<strong><font color="red">17,000 reads</font></strong>, in Chinese): 
			<img class="img-thumbnail" src="image/liangziwei.jpg" alt="" height="16"><a href="https://mp.weixin.qq.com/s/zGBpTyycIMCkXJBrEdYdfA"> QbitAI (ÈáèÂ≠ê‰Ωç)</a>; 
			<img class="img-thumbnail" src="image/jiqizhixin.jpg" alt="" style="" height="16"><a href="https://mp.weixin.qq.com/s/4Tp5ReW0tHozWX1hKexzEQ"> Synced (Êú∫Âô®‰πãÂøÉ)</a>.</p>
      </td>
    </tr>
   </table>
    

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
      <a href="paper/ACL2021_CMCL/framework.png"><img src='paper/ACL2021_CMCL/framework.png'  width="200" height="110"></a>
      </td>
      <td valign="top" width="75%">
	 <a name="[4]"><big>[4]</big></a> <em>Competence-based Multimodal Curriculum Learning for Medical Report Generation</em><br>
     <strong>Fenglin Liu</strong>, Shen Ge, Xian Wu<br>
        In Proceedings of <font color="#a82e2e"><strong>ACL 2021</font> (<font color="red">Oral</font>)</strong>,
              <a href="https://aclanthology.org/2021.acl-long.234/">[paper link]</a>, 
	      <a href="paper/ACL2021_CMCL/ACL2021_CMCL.pdf">[pdf]</a>, 
	      <a href="paper/ACL2021_CMCL/ACL2021_CMCL_ppt.pdf">[ppt]</a>, 
	      <a href="https://drive.google.com/file/d/1ngMdclHQ-V-_g87xW8LZk8wlcY_3qxBo/view?usp=sharing">[video]</a><br>
	      
        <p></p>
		<p>&bull; We introduce the competence-based multimodal curriculum learning in medical report generation, 
			which enables the models to gradually proceed from easy samples to more complex ones in training, 
			alleviating the data bias and thus improving the training efficiency.</p>
      </td>
    </tr>
   </table>

<!--
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
       <a href="paper/ACL2021_CA/contrastive.png"><img src='paper/ACL2021_CA/contrastive.png'  width="200" height="110"></a>
      </td>
      <td valign="top" width="75%">
	 <a name="[8]"><big>[8]</big></a> <em>Contrastive Attention for Automatic Chest X-ray Report Generation</em><br>
     <strong>Fenglin Liu</strong>, Changchang Yin, Xian Wu, Shen Ge, Ping Zhang, Xu Sun<br>
        In Findings of <strong><font color="#a82e2e">ACL 2021</font></strong>
	      <a href="https://aclanthology.org/2021.findings-acl.23/">[paper link]</a>, 
	      <a href="paper/ACL2021_CA/ACL2021_CA.pdf">[pdf]</a>, <a href="paper/ACL2021_CA/ACL2021_CA_ppt.pdf">[ppt]</a>, 
	      <a href="paper/ACL2021_CA/ACL2021_CA_poster.pdf">[poster]</a>, 
	      <a href="https://drive.google.com/file/d/1QKLAHdUhyqeoigqYRFB3s5qmbW6c0Nko/view?usp=sharing">[video]</a><br>

        <p></p>
		<p>&bull; We propose the Contrastive Attention model to capture abnormal regions by contrasting the input image and normal images to distill the contrastive information, which can better represent the visual features of abnormal regions and thus improve the performance of Chest X-ray report generation models. </p>
      </td>
    </tr>
   </table>
-->

<!--
      <table width="100%" align="center" border="1" cellspacing="0" cellpadding="20">
 
          <strong><big>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&bull; Area: &nbsp; Machine Learning</big></strong> [2019 ‚Äì Present]
 
      </table>
-->



    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
       <a href="paper/NeurIPS2020_Prophet/framework.png"><img src='paper/NeurIPS2020_Prophet/framework.png'  width="200" height="110"></a>
      </td>
      <td valign="top" width="75%">
	<a name="[3]"><big>[3]</big></a> <em>Prophet Attention: Predicting Attention with Future Attention</em><br>
       <strong>Fenglin Liu</strong>, Xuancheng Ren, Xian Wu, Shen Ge, Wei Fan, Yuexian Zou, Xu Sun <br>
        In Proceedings of <font color="#a82e2e"><strong>NeurIPS 2020</strong></font>,
	      <a href="https://proceedings.neurips.cc/paper/2020/file/13fe9d84310e77f13a6d184dbf1232f3-Paper.pdf">[paper link]</a>,
	      <a href="paper/NeurIPS2020_Prophet/NeurIPS2020_Prophet.pdf">[pdf]</a>,
	      <a href="paper/NeurIPS2020_Prophet/NeurIPS2020_Prophet_ppt.pdf">[ppt]</a>,
	      <a href="paper/NeurIPS2020_Prophet/NeurIPS2020_Poster.pdf">[poster]</a>
	      <br>
        <p></p>
		<p> &bull; We propose the Prophet Attention to calculate attentional weights based on future information,
			and force the attention model to learn to correctly ground each generated word to proper image regions.<br/>
			&bull; <font color="red"><strong>Leaderboard #1</strong></font> on Microsoft COCO Image Captioning Challenge from June 2, 2020 to September 3, 2020. </p>
      </td>
    </tr>
   </table>

<!--
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
       <a href="paper/AAAI2020_aimNet/framework.png"><img src='paper/AAAI2020_aimNet/framework.png'  width="200" height="110"></a>
      </td>
      <td valign="top" width="75%">
	 <a name="[4]"><big>[4]</big></a> <em>Federated Learning for Vision-and-Language Grounding Problems</em><br>
       <strong>Fenglin Liu</strong>, Xian Wu, Shen Ge, Wei Fan, Yuexian Zou <br>
        In Proceedings of <font color="#a82e2e"><strong>AAAI 2020</font> (<font color="red">Oral</font>)</strong>,
	      <a href="https://ojs.aaai.org//index.php/AAAI/article/view/6824">[paper link]</a>,
	      <a href="paper/AAAI2020_aimNet/AAAI2020_aimNet.pdf">[pdf]</a>,
	      <a href="paper/AAAI2020_aimNet/AAAI2020_aimNet_ppt.pdf">[ppt]</a>
	      <br>
        <p></p>
	      <p> &bull; We propose a federated learning framework to obtain various types of image representations from different vision-and-language tasks 
	      without the sharing of downstream task data, which are then fused together to form fine-grained image representations.
	      The representations merge useful features from different tasks, 
	      and are thus much more powerful than the original representations alone in individual tasks. </p>
      </td>
    </tr>
   </table>
-->

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
       <a href="paper/NeurIPS2019_MIA/visualization.png"><img src='paper/NeurIPS2019_MIA/visualization.png' width="200" height="110"></a>
      </td>
      <td valign="top" width="75%">
	 <a name="[2]"><big>[2]</big></a> <em>Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations</em><br>
       <strong>Fenglin Liu</strong>, Yuanxin Liu, Xuancheng Ren, Xiaodong He, Xu Sun <br>
        In Proceedings of <font color="#a82e2e"><strong>NeurIPS 2019</strong></font>,
	      <a href="https://proceedings.neurips.cc/paper/2019/file/9fe77ac7060e716f2d42631d156825c0-Paper.pdf">[paper link]</a>,
	      <a href="paper/NeurIPS2019_MIA/NeurIPS2019_MIA.pdf">[pdf]</a>,
	      <a href="paper/NeurIPS2019_MIA/NeurIPS2019_MIA_Poster.pdf">[poster]</a>,
	      <a href="https://github.com/fenglinliu98/MIA">[code]</a>
	      <br>
        <p></p>
		<p> &bull; We aim at representing an image with a set of integrated visual regions and corresponding textual concepts, 
			reflecting certain semantics. To this end, we build the Mutual Iterative Attention (MIA) module, 
			which integrates correlated visual features and textual concepts, respectively, by aligning the two modalities. </p>
      </td>
    </tr>
   </table>

<!--
      <table width="100%" align="center" border="1" cellspacing="0" cellpadding="20">
    
          <strong><big>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &bull; Area: &nbsp; Multimodal NLP  </big></strong> [2017 ‚Äì Present]
 
      </table>
-->

<!--
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
       <a href="paper/ACL2021_O2NA/introduction.png"><img src='paper/ACL2021_O2NA/introduction.png'  width="200" height="110"></a>
      </td>
      <td valign="top" width="75%">
	 <a name="[4]"><big>[4]</big></a> <em>O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning</em><br>
	      <strong>Fenglin Liu</strong>, Xuancheng Ren, Xian Wu, Bang Yang, Shen Ge, Xu Sun <br>
        In Findings of <strong><font color="#a82e2e">ACL 2021</font></strong>,
	      <a href="https://aclanthology.org/2021.findings-acl.24/">[paper link]</a>,
	      <a href="https://aclanthology.org/2021.findings-acl.24.pdf">[pdf]</a>,
	      <a href="https://github.com/yangbang18/Non-Autoregressive-Video-Captioning">[code]</a>
	      <br>
        <p></p> 
	      <p> &bull; We introduce the problem of controllable video captioning in the sense of controlled contents, 
		      which has more practical values than the existing studies on syntactic variations.  
		      We propose the O2NA, which is based on the non-autoregressive decoding method and encouraged to describe the focused objects user cares about. <br/>
		  &bull; <font color="red"><strong>3x faster inference speed</strong></font> than previous works. </p>
	      </p> 
      </td>
    </tr>
   </table>
-->




<!--
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
      <td width="15%">
       <a href="paper/IJCAI2019_GLIED/visualization.png"><img src='paper/IJCAI2019_GLIED/visualization.png'  width="200" height="110"></a>
      </td>
      <td valign="top" width="75%">
	 <a name="[2]"><big>[2]</big></a> <em>Exploring and Distilling Cross-Modal Information for Image Captioning</em><br>
       <strong>Fenglin Liu</strong>, Xuancheng Ren, Yuanxin Liu, Kai Lei, Xu Sun <br>
        In Proceedings of <font color="#a82e2e"><strong>IJCAI 2019</font> (<font color="red">Oral</font>)</strong>,
	      <a href="https://www.ijcai.org/proceedings/2019/708">[paper link]</a>,
	      <a href="paper/IJCAI2019_GLIED/IJCAI2019_GLIED.pdf">[pdf]</a>,
	      <a href="paper/IJCAI2019_GLIED/IJCAI2019_GLIED_ppt.pdf">[ppt]</a>
	      <br>
        <p></p>
		<p> &bull;  We propose an approach to <em>globally</em> "captures the inherent spatial and relational groupings of the individual image regions and attribute words 
			for an aspect-based image representation", and <em>locally</em> "extracts fine-grained source information for precise and accurate word selection". <br/>
			&bull; <font color="red"><strong>State-of-the-art</strong></font> on Image Captioning 
			with <font color="red"><strong>fewer parameters</strong></font> and <font color="red"><strong>faster computation</strong></font>. </p>
      </td>
    </tr>
   </table>
-->




-->			

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<br>
				<p align="middle"><font size="1">
				<a href="http://www.clustrmaps.com/map/Paperlist.site" title="Visit tracker for Paperlist.site">
					<img src="//www.clustrmaps.com/map_v2.png?d=aahU2d7YM1co9Ob0B_5uxAcswOX3gGWBAlxDpPjsmy8" /></a>
				<br/> <br/>
			
				</tbody></table>
<!---->
	
				
				
				
<!--
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<br>
				<p align="middle"><font size="2">
				This awesome template was borrowed from  <a href="https://people.eecs.berkeley.edu/~barron/">this guy</a>~
				</tbody></table>
				href="" 
-->
				
				
  </body>
</html>
